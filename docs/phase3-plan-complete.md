# План реализации Этапа 3: Расширенный анализ

## Цели этапа

- Реализация распознавания именованных сущностей (NER) с поддержкой разных языков
- Разработка системы тематического моделирования для определения тем документов
- Создание механизмов выявления версий и дедупликации на уровне текста
- Анализ и извлечение взаимосвязей между документами
- Подготовка данных для будущей графовой базы (Neo4j)
- Расширение возможностей поиска на основе аналитических данных

## Детальные задачи и требования

### 1. Расширение схемы базы данных

**Задача**: Разработать и внедрить расширения схемы SQLite для поддержки сущностей, тем, версий и связей.

**Требования**:
- Проектирование и добавление новых таблиц:
  ```sql
  -- Таблица сущностей
  CREATE TABLE entities (
      id INTEGER PRIMARY KEY,
      text TEXT,                   -- Оригинальный текст сущности
      normalized_text TEXT,        -- Нормализованная форма
      type TEXT,                   -- Тип сущности (PERSON, ORG, и т.д.)
      language TEXT,               -- Язык сущности
      created_at TIMESTAMP
  );
  
  -- Таблица вхождений сущностей в чанки
  CREATE TABLE chunk_entities (
      id INTEGER PRIMARY KEY,
      chunk_id INTEGER,            -- Ссылка на чанк
      entity_id INTEGER,           -- Ссылка на сущность
      position INTEGER,            -- Позиция в чанке
      context TEXT,                -- Контекст сущности (окружающий текст)
      confidence REAL,             -- Уверенность в определении
      FOREIGN KEY (chunk_id) REFERENCES chunks(id),
      FOREIGN KEY (entity_id) REFERENCES entities(id)
  );
  
  -- Таблица тем/категорий
  CREATE TABLE topics (
      id INTEGER PRIMARY KEY,
      name TEXT,                   -- Название темы
      description TEXT,            -- Описание темы
      parent_id INTEGER,           -- Родительская тема (для иерархии)
      keywords TEXT,               -- Ключевые слова темы (JSON)
      created_at TIMESTAMP,
      FOREIGN KEY (parent_id) REFERENCES topics(id)
  );
  
  -- Таблица связей документов с темами
  CREATE TABLE document_topics (
      id INTEGER PRIMARY KEY,
      file_id INTEGER,             -- Ссылка на документ
      topic_id INTEGER,            -- Ссылка на тему
      relevance REAL,              -- Релевантность темы к документу (0.0-1.0)
      FOREIGN KEY (file_id) REFERENCES files(id),
      FOREIGN KEY (topic_id) REFERENCES topics(id)
  );
  
  -- Таблица связей между сущностями
  CREATE TABLE entity_relations (
      id INTEGER PRIMARY KEY,
      source_entity_id INTEGER,    -- Ссылка на исходную сущность
      target_entity_id INTEGER,    -- Ссылка на целевую сущность
      relation_type TEXT,          -- Тип связи
      confidence REAL,             -- Уверенность в связи
      FOREIGN KEY (source_entity_id) REFERENCES entities(id),
      FOREIGN KEY (target_entity_id) REFERENCES entities(id)
  );
  
  -- Расширение таблицы file_versions для версионности
  ALTER TABLE file_versions 
  ADD COLUMN version_type TEXT;    -- Тип версии (draft, final, etc.)
  ALTER TABLE file_versions 
  ADD COLUMN diff_summary TEXT;    -- Краткое описание изменений
  ```

- Создание миграций для обновления существующей базы
- Разработка моделей SQLAlchemy для новых таблиц
- Создание индексов для оптимизации запросов:
  ```sql
  CREATE INDEX idx_entities_type ON entities(type);
  CREATE INDEX idx_entities_normalized_text ON entities(normalized_text);
  CREATE INDEX idx_chunk_entities_chunk_id ON chunk_entities(chunk_id);
  CREATE INDEX idx_chunk_entities_entity_id ON chunk_entities(entity_id);
  CREATE INDEX idx_document_topics_file_id ON document_topics(file_id);
  CREATE INDEX idx_document_topics_topic_id ON document_topics(topic_id);
  ```

### 2. Распознавание именованных сущностей (NER)

**Задача**: Разработать модуль для выявления, классификации и нормализации именованных сущностей в документах.

**Требования**:
- Поддержка многоязычного распознавания с фокусом на русский и английский языки
- Выявление основных типов сущностей:
  - Персоны (PERSON)
  - Организации (ORG)
  - Локации (LOC/GPE)
  - Даты и временные периоды (DATE/TIME)
  - Продукты и технологии (PRODUCT/TECH)
  - Числовые данные (MONEY, PERCENT, QUANTITY)
- Архитектура модуля:
  - Базовый класс `EntityExtractor` с абстрактными методами
  - Реализации для различных языков и библиотек
  - Фабрика экстракторов для выбора подходящего на основе языка
  - Сервис извлечения для координации процесса
- Нормализация сущностей:
  - Приведение к канонической форме (нормализация падежей, числа)
  - Разрешение кореферентности (связывание различных упоминаний одной сущности)
  - Сопоставление вариантов написания (аббревиатуры, сокращения)
- Процесс обработки:
  1. Определение языка чанка
  2. Выбор соответствующего экстрактора
  3. Извлечение и классификация сущностей
  4. Нормализация сущностей
  5. Сохранение в базе данных с метаинформацией
- Оптимизация:
  - Пакетная обработка чанков
  - Кэширование для повторяющихся фрагментов
  - Параллельная обработка для повышения производительности

**Используемые библиотеки**:
- `spaCy` с моделями `ru_core_news_lg` и `en_core_web_lg` как основное решение
- `natasha` как специализированный инструмент для русского языка (опционально)
- `langid` или `fasttext` для определения языка
- `pandas` для обработки наборов сущностей

**Код**:
```python
# Пример базового класса для извлечения сущностей
class EntityExtractor:
    def extract_entities(self, text: str) -> List[Entity]:
        """Извлекает именованные сущности из текста.
        
        Args:
            text: Текст для анализа
            
        Returns:
            Список извлеченных сущностей
        """
        raise NotImplementedError
    
    def normalize_entity(self, entity: Entity) -> Entity:
        """Нормализует сущность.
        
        Args:
            entity: Сущность для нормализации
            
        Returns:
            Нормализованная сущность
        """
        raise NotImplementedError
    
    @classmethod
    def supports_language(cls, language: str) -> bool:
        """Проверяет, поддерживает ли экстрактор указанный язык.
        
        Args:
            language: Код языка (например, 'ru', 'en')
            
        Returns:
            True, если язык поддерживается, иначе False
        """
        raise NotImplementedError
```

### 3. Тематическое моделирование

**Задача**: Разработать систему для определения основных тем документов, извлечения ключевых слов и классификации.

**Требования**:
- Извлечение ключевых слов и фраз:
  - Методы на основе статистики (TF-IDF, TextRank)
  - Методы на основе эмбеддингов (KeyBERT)
  - Комбинированные подходы для повышения качества
- Определение тем документов:
  - Кластеризация документов по семантической близости
  - Создание иерархии тем (родительские и дочерние)
  - Автоматическое присвоение названий темам на основе ключевых слов
- Классификация документов:
  - Возможность создания пользовательских категорий
  - Обучение классификаторов на основе примеров
  - Полуавтоматическая классификация с обратной связью
- Процесс обработки:
  1. Извлечение ключевых слов из документов
  2. Кластеризация документов по эмбеддингам
  3. Определение и именование тем
  4. Присвоение документам тем с оценкой релевантности
  5. Сохранение результатов в базе данных
- Оптимизация:
  - Инкрементальное обновление при добавлении новых документов
  - Кэширование промежуточных результатов
  - Параллельная обработка для больших корпусов

**Используемые библиотеки**:
- `keybert` - для извлечения ключевых слов на основе эмбеддингов
- `BERTopic` - для тематического моделирования и кластеризации
- `scikit-learn` - для традиционных методов и классификации
- `umap-learn` - для снижения размерности (используется в BERTopic)
- `hdbscan` - для кластеризации (используется в BERTopic)

**Архитектура**:
- Модуль извлечения ключевых слов с поддержкой разных алгоритмов
- Модуль тематического моделирования с настраиваемыми параметрами
- Модуль классификации для пользовательских категорий
- Сервис для управления всем процессом тематического анализа

### 4. Определение версий и дедупликация

**Задача**: Разработать механизмы для выявления различных версий документов, определения дубликатов и отслеживания изменений.

**Требования**:
- Уровни обнаружения дубликатов:
  - **Точные дубликаты**: совпадение хеша содержимого файла
  - **Контентные дубликаты**: совпадение хеша извлеченного текста
  - **Семантические дубликаты**: высокая семантическая близость
  - **Частичные дубликаты**: значительное перекрытие содержимого
- Определение версий документов:
  - Анализ имен файлов на наличие патернов версий (v1, draft, final)
  - Временная последовательность создания/изменения
  - Семантическое сходство содержимого
  - Структурное сравнение (диффы)
- Построение цепочек версий:
  - Определение базовой/исходной версии
  - Выстраивание хронологии изменений
  - Идентификация финальной версии
- Анализ изменений:
  - Сравнение текстов для выявления добавленных/удаленных/измененных фрагментов
  - Оценка степени изменений (минорные/мажорные)
  - Генерация краткого описания изменений
- Процесс обработки:
  1. Группировка документов по потенциальной связанности
  2. Определение точных и контентных дубликатов
  3. Выявление семантически похожих документов
  4. Анализ временной последовательности и структуры
  5. Построение графа версий
  6. Сохранение информации о версиях и дубликатах в базе данных

**Используемые библиотеки**:
- `difflib` - для построчного сравнения текстов
- `rapidfuzz` - для нечеткого сравнения строк
- `networkx` - для построения графа версий
- Собственная реализация на основе эмбеддингов из Этапа 2

**Алгоритмы**:
- Хеширование для точных дубликатов
- Косинусное сходство векторов для семантических дубликатов
- Sequence Matcher для структурного сравнения
- Алгоритмы поиска сообществ в графе для группировки версий

### 5. Анализ взаимосвязей

**Задача**: Разработать методы для выявления и анализа взаимосвязей между документами, сущностями и темами.

**Требования**:
- Типы выявляемых связей:
  - **Документ → Сущность**: документ содержит сущность
  - **Документ → Документ**: ссылки, совместное упоминание, семантическая близость
  - **Сущность → Сущность**: совместное упоминание, контекстная близость
  - **Документ → Тема**: документ относится к теме
  - **Временные связи**: последовательность создания, версионность
- Механизмы выявления связей:
  - Анализ совместного упоминания сущностей
  - Семантическая близость документов на основе эмбеддингов
  - Наличие прямых ссылок или упоминаний
  - Временная близость создания/редактирования
- Построение графа связей:
  - Документы, сущности и темы как узлы
  - Типизированные и взвешенные связи
  - Метаданные узлов и связей
- Анализ графа:
  - Выявление сообществ (кластеров)
  - Определение центральных узлов (PageRank, Betweenness Centrality)
  - Поиск путей между узлами
- Подготовка данных для Neo4j:
  - Форматирование данных для импорта
  - Сохранение идентификаторов для связи с SQLite
  - Определение схемы и индексов Neo4j

**Используемые библиотеки**:
- `networkx` - для построения и анализа графов
- `community` (python-louvain) - для выявления сообществ
- `pandas` - для подготовки данных
- `sklearn` - для дополнительного анализа

**Алгоритмы**:
- Louvain или Leiden для обнаружения сообществ
- PageRank для определения важности узлов
- Betweenness Centrality для выявления узлов-мостов
- Shortest Path для поиска связей между узлами

### 6. Расширение возможностей поиска

**Задача**: Интегрировать результаты расширенного анализа в поисковую систему для улучшения релевантности и функциональности.

**Требования**:
- Расширение поискового API:
  - Поиск по сущностям (`entity:ИмяСущности type:PERSON`)
  - Фильтрация по темам (`topic:ИмяТемы`)
  - Поиск по версиям (`version:final`, `version:latest`)
  - Поиск по связям (`related_to:ИдДокумента`)
- Улучшение ранжирования результатов:
  - Учет центральности документов в графе
  - Учет релевантности тем
  - Предпочтение финальных версий
  - Объединение различных факторов релевантности
- Интерфейс для сложных запросов:
  - Комбинированные запросы с логическими операторами
  - Фасетный поиск с фильтрами
  - Поиск по шаблонам
- Продвинутые возможности:
  - Поиск документов, где две сущности упоминаются вместе
  - Поиск по семантической близости с учетом графовых связей
  - Поиск по временным последовательностям

**Архитектура**:
- Расширенный парсер запросов
- Модуль построения сложных запросов к базам данных
- Система ранжирования с комбинированием нескольких метрик
- API для интеграции с интерфейсом пользователя

**Примеры запросов**:
- Найти все документы с упоминанием персоны "Иванов" в контексте организации "Рога и Копыта"
- Найти последние версии документов, относящихся к теме "Бюджетирование"
- Найти документы, семантически близкие к заданному, но с упоминанием конкретной сущности

## Технические детали реализации

### Схема работы расширенного анализа

```
┌─────────────────┐    ┌────────────────────┐    ┌─────────────────┐
│                 │    │                    │    │                 │
│  База SQLite    │───▶│  Модули анализа    │───▶│  Обновленная БД │
│  (Этап 2)       │    │                    │    │  + Графовые     │
│                 │    │                    │    │  структуры      │
└─────────────────┘    └────────────────────┘    └─────────────────┘
                              │
                              │
                              ▼
┌─────────────────┐    ┌────────────────────┐    ┌─────────────────┐
│                 │    │                    │    │                 │
│  Расширенный    │◀───│  Интеграция в      │◀───│  Подготовка     │
│  поиск          │    │  поисковую систему │    │  данных для     │
│                 │    │                    │    │  Neo4j (Этап 4) │
└─────────────────┘    └────────────────────┘    └─────────────────┘
```

### Алгоритм обработки документа

1. Извлечение сущностей:
   - Загрузка документа из БД
   - Определение языка
   - Применение соответствующего NER-экстрактора
   - Нормализация и дедупликация сущностей
   - Сохранение в БД

2. Тематическое моделирование:
   - Извлечение ключевых слов
   - Обновление общей модели тем (при необходимости)
   - Определение тем документа
   - Сохранение связей документ-тема

3. Версионный анализ:
   - Поиск потенциальных дубликатов/версий
   - Сравнение контента
   - Определение типа связи (дубликат/версия)
   - Обновление графа версий

4. Анализ взаимосвязей:
   - Выявление связей через общие сущности
   - Определение семантически близких документов
   - Построение графа связей
   - Вычисление метрик центральности

5. Интеграция с поиском:
   - Обновление поисковых индексов
   - Настройка фильтров и фасетов
   - Обновление ранжирования

## План реализации

### Неделя 1: Расширение схемы БД и реализация NER (8-10 дней)

1. **День 1-3**: Проектирование и расширение схемы базы данных
   - Добавление таблиц для сущностей и их связей
   - Добавление таблиц для тем и ключевых слов
   - Расширение таблиц для версий и отношений между документами
   - Создание миграций и моделей SQLAlchemy

2. **День 4-7**: Реализация базового NER
   - Интеграция spaCy с моделями для русского и английского языков
   - Разработка системы извлечения и классификации сущностей
   - Создание модуля нормализации сущностей
   - Сохранение результатов в базе данных

3. **День 8-10**: Тестирование и оптимизация NER
   - Тестирование на различных документах
   - Оптимизация производительности
   - Улучшение точности распознавания
   - Документирование компонента

### Неделя 2: Тематическое моделирование и дедупликация (8-10 дней)

1. **День 1-4**: Разработка модуля тематического моделирования
   - Реализация извлечения ключевых слов
   - Интеграция BERTopic для моделирования тем
   - Разработка системы классификации документов
   - Сохранение результатов в базе данных

2. **День 5-10**: Реализация системы определения версий и дедупликации
   - Разработка механизмов сравнения документов
   - Реализация выявления цепочек версий
   - Создание интерфейса для управления версиями
   - Тестирование на наборе с различными версиями документов

### Неделя 3: Анализ взаимосвязей и интеграция (8-10 дней)

1. **День 1-5**: Разработка модуля анализа взаимосвязей
   - Создание алгоритмов выявления связей между документами
   - Построение и анализ графа документов и сущностей
   - Подготовка данных для Neo4j
   - Сохранение результатов в базе данных

2. **День 6-10**: Интеграция компонентов и расширение поиска
   - Объединение результатов NER, тематического моделирования и анализа связей
   - Расширение поискового API
   - Улучшение ранжирования результатов
   - Расширение CLI для новых функций

### Неделя 4-5: Тестирование, оптимизация и документация (10-12 дней)

1. **День 1-4**: Комплексное тестирование
   - Тестирование на большом наборе данных
   - Выявление и исправление ошибок
   - Оптимизация производительности
   - Стресс-тестирование компонентов

2. **День 5-8**: Оптимизация и улучшения
   - Оптимизация использования памяти
   - Улучшение производительности критических компонентов
   - Реализация дополнительных функций
   - Повышение надежности системы

3. **День 9-12**: Документация и переход к следующему этапу
   - Завершение документации
   - Подготовка к Этапу 4
   - Обзор проделанной работы
   - Определение приоритетов для следующего этапа

## Критические аспекты реализации

### 1. Оптимизация производительности NER

Распознавание именованных сущностей может быть ресурсоемким:
- Использовать пакетную обработку для spaCy
- Рассмотреть возможность параллельной обработки нескольких документов
- Кэшировать результаты для повторяющихся фрагментов
- Для очень больших корпусов - использовать более легкие модели с балансом точность/скорость

### 2. Качество тематического моделирования

Для получения осмысленных тем необходимо:
- Тщательно подбирать параметры BERTopic (количество соседей, минимальный размер кластера)
- Использовать предобработку текста (удаление стоп-слов, лемматизация)
- Регулярно переобучать модель на расширяющемся корпусе
- Предусмотреть возможность ручной корректировки тем

### 3. Сложность определения версий

Выявление действительно связанных версий требует комбинированного подхода:
- Анализировать название файла и метаданные
- Учитывать временную последовательность
- Использовать как структурное, так и семантическое сравнение
- Предоставлять механизм ручного уточнения версий

### 4. Масштабируемость графового анализа

При увеличении количества документов и сущностей:
- Ограничивать размер анализируемого графа (например, по времени или теме)
- Использовать методы разбиения графа на подграфы
- Оптимизировать алгоритмы для работы с разреженными графами
- Сохранять промежуточные результаты анализа

## Ожидаемые результаты

По завершении Этапа 3 мы получим:

1. Расширенную базу данных с сущностями, темами и связями
2. Систему распознавания и нормализации сущностей
3. Механизмы тематического моделирования и классификации
4. Инструменты для определения версий и дубликатов
5. Систему анализа взаимосвязей между документами
6. Расширенный поиск с учетом сущностей, тем и связей
7. Подготовленные данные для загрузки в Neo4j на следующем этапе

## Метрики успеха

- **NER**: правильное распознавание >80% сущностей в тестовых документах
- **Тематическое моделирование**: корректное определение основных тем у >85% документов
- **Версионирование**: выявление >90% точных дубликатов и >70% семантически близких версий
- **Анализ взаимосвязей**: обнаружение связей между документами с точностью >75%
- **Поиск**: улучшение релевантности результатов на 30% по сравнению с базовым семантическим поиском
- **Производительность**: общее время обработки - не более 2x от времени базовой индексации

## Потенциальные риски и их минимизация

1. **Качество NER на сложных доменных текстах**
   - **Риск**: Стандартные модели могут плохо работать со специфической терминологией
   - **Решение**: Рассмотреть возможность дообучения моделей на специфических данных или использования правил

2. **Производительность при большом количестве документов**
   - **Риск**: Время обработки может расти нелинейно при увеличении корпуса
   - **Решение**: Оптимизация алгоритмов, инкрементальная обработка, распределение вычислений

3. **Языковые ограничения**
   - **Риск**: Разное качество обработки для русского и английского языков
   - **Решение**: Тщательный подбор моделей для каждого языка, нормализация качества

4. **Сложность интеграции результатов разных модулей**
   - **Риск**: Несогласованность между результатами NER, тематического моделирования и версионирования
   - **Решение**: Разработка унифицированной системы метаданных, тестирование интеграции

5. **Субъективность оценки тем и связей**
   - **Риск**: Автоматически определенные темы и связи могут не соответствовать ожиданиям пользователя
   - **Решение**: Возможность ручной корректировки, механизмы обратной связи
