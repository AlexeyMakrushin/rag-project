# Общий план проекта виртуальной базы знаний (ВБЗ)

## Концепция и цели

Проект направлен на создание системы виртуальной базы знаний (ВБЗ), которая индексирует и анализирует оригинальные файлы различных форматов без создания их дублирующих копий. Система извлекает текст и метаданные, сохраняет их в структурированном формате, индексирует для семантического поиска и анализирует для выявления взаимосвязей.

### Основные цели проекта:

1. **Централизация и поисковая доступность** – организация и обеспечение поиска по документам из разных источников (сообщения мессенджеров, рабочие файлы и т.д.)
2. **Выявление дубликатов и версий** – определение различных версий одного документа и устранение избыточности
3. **Семантический поиск и анализ** – поиск по смыслу и выявление связей между документами
4. **Инкрементальное обновление** – автоматическое обновление индекса при изменении оригинальных файлов

## Общая архитектура системы

### Компоненты системы:

1. **Сканер файловой системы** - обнаружение и отслеживание файлов
2. **Система извлечения текста** - извлечение текста и метаданных из разных форматов
3. **База метаданных SQLite** - хранение информации о файлах, извлеченном тексте и чанках (с возможностью миграции на PostgreSQL при необходимости)
4. **Векторное хранилище Qdrant** - хранение и поиск эмбеддингов для семантического поиска
5. **Анализатор содержимого** - NER, тематическое моделирование, извлечение ключевых понятий
6. **Графовая база Neo4j** (на более позднем этапе) - хранение и анализ взаимосвязей
7. **Интерфейс пользователя** - CLI/web-интерфейс для управления системой

## Выбранные технологии и библиотеки

### Базовые компоненты:
- **Python** - основной язык разработки
- **SQLite** - хранение метаданных и извлеченного текста (с закладкой под миграцию на PostgreSQL)
- **Qdrant** - векторное хранилище 
- **Neo4j** - графовая база данных (будет добавлена позже)

### Библиотеки:
- **SQLAlchemy** - ORM для работы с базами данных (обеспечит легкую миграцию с SQLite на PostgreSQL)
- **watchdog/cron** - мониторинг изменений в файловой системе / периодическое сканирование
- **unstructured/Docling** - универсальные экстракторы текста
- **PyMuPDF** (fitz) - для более тонкой работы с PDF-документами
- **sentence-transformers** - создание эмбеддингов (предпочтительнее fastembed из-за гибкости)
- **spaCy** - NER и анализ текста для разных языков
- **llama-index/langchain** - разбиение на чанки, оркестрация компонентов
- **Typer/FastAPI** - создание CLI/веб-интерфейса

## Этапы реализации проекта

### Этап 1: Базовая инфраструктура (3-4 недели)
- Создание сканера директорий и файлов
- Проектирование и настройка детальной схемы SQLite
- Реализация базовых экстракторов текста с механизмами оценки качества извлечения
- Создание CLI для управления процессами
- Разработка системы логирования и мониторинга

### Этап 2: Базовая индексация (4-6 недель)
- Проектирование и добавление таблиц для чанков
- Разработка стратегий разбиения текста (по размеру, семантические границы)
- Генерация и хранение эмбеддингов с информацией о версии модели
- Настройка Qdrant для векторного поиска
- Базовый семантический поиск

### Этап 3: Расширенный анализ (5-7 недель)
- Интеграция NER с поддержкой разных языков
- Тематическое моделирование и классификация документов
- Разработка алгоритмов определения версий и дедупликации на уровне текста
- Выявление и анализ взаимосвязей между документами
- Подготовка структур данных для будущей графовой базы

### Этап 4: Графовые представления и интерфейс (6-8 недель)
- Добавление Neo4j для хранения графа
- Визуализация связей между документами
- Разработка веб-интерфейса (начиная с Streamlit для прототипа)
- Интеграция с RAG-системами
- Оптимизация системы и масштабирование

## Ключевые аспекты и особенности

### 1. Двухэтапный процесс обработки

**Преимущества**:
- Извлечение текста происходит однократно, результаты сохраняются в SQLite
- Дальнейший анализ работает с извлеченным текстом, не обращаясь к оригиналам
- Возможность переиндексации с новыми моделями без повторного извлечения текста

### 2. Схема базы данных

Ключевые таблицы и особенности:
- `files` - информация о файлах, включая хеш содержимого и качество извлечения текста
- `content` - извлеченный текст/JSON-структура с сохранением форматирования
- `chunks` - чанки текста с информацией о позиции в документе, языке и т.д.
- `entities` - извлеченные сущности с нормализованной формой
- `topics` - темы документов и чанков
- `relationships` - связи между документами (для подготовки к Neo4j)

### 3. Стратегии хранения и обновления

- Хранение как SHA-256 хеша файла, так и хеша текстового содержимого
- Отслеживание версий моделей эмбеддингов для возможности переиндексации
- Инкрементальное обновление с удалением устаревших данных по всему конвейеру
- Периодические бэкапы SQLite и Qdrant для защиты от потери данных

### 4. Обработка сложных случаев

- Отметка о качестве извлечения текста для выявления проблемных документов
- Поддержка многоязычных документов (детекция языка на уровне чанков)
- Семантическое разбиение длинных документов с сохранением смысловых границ
- Обработка версий и "почти дубликатов" документов

## Технические детали и уточнения

### Выбор моделей для эмбеддингов:

- **Основной выбор**: sentence-transformers с моделями типа `paraphrase-multilingual-mpnet-base-v2` для поддержки русского и английского языков
- **Альтернатива**: возможность переключения на fastembed, если производительность станет критичной
- **Размерность векторов**: динамическая, зависит от выбранной модели

### Стратегия индексации:

- Периодическое полное сканирование (например, раз в сутки) как основной метод
- Возможность реактивного мониторинга изменений через watchdog для важных директорий
- Разделение процессов: сканирование → извлечение → индексация (возможность запуска по отдельности)

### Масштабирование:

- SQLite для MVP, с заделом на миграцию на PostgreSQL (через SQLAlchemy)
- Qdrant в Docker-контейнере с настройками производительности
- Метрики и мониторинг для выявления узких мест

## Следующие шаги

1. Детальное проектирование схемы базы данных SQLite
2. Разработка и тестирование компонентов сканирования и извлечения текста
3. Создание тестового набора данных с различными форматами и сценариями
4. Последовательная реализация этапов с приоритетом качества над скоростью разработки
